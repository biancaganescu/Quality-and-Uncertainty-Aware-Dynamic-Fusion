Loaded split indices from split_indices.pth
Epoch 0 train loss: tensor(0.2243, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 0 valid loss: tensor(0.1888, device='cuda:0') f1_micro: 0.6014150943396227 f1_macro: 0.058945908460471576
Saving Best
Epoch 1 train loss: tensor(0.1773, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 1 valid loss: tensor(0.1711, device='cuda:0') f1_micro: 0.6044776119402986 f1_macro: 0.07421972430521225
Saving Best
Epoch 2 train loss: tensor(0.1642, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 2 valid loss: tensor(0.1718, device='cuda:0') f1_micro: 0.613365155131265 f1_macro: 0.08355330823201079
Saving Best
Epoch 3 train loss: tensor(0.1515, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 3 valid loss: tensor(0.1718, device='cuda:0') f1_micro: 0.6071871127633209 f1_macro: 0.09591116427228531
Saving Best
Epoch 4 train loss: tensor(0.1396, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 4 valid loss: tensor(0.1681, device='cuda:0') f1_micro: 0.6437869822485207 f1_macro: 0.12505530081683594
Saving Best
Epoch 5 train loss: tensor(0.1305, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 5 valid loss: tensor(0.1773, device='cuda:0') f1_micro: 0.6340909090909091 f1_macro: 0.1415381988365677
Saving Best
Epoch 6 train loss: tensor(0.1193, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 6 valid loss: tensor(0.1717, device='cuda:0') f1_micro: 0.6166868198307135 f1_macro: 0.17107536903834683
Saving Best
Epoch 7 train loss: tensor(0.1093, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 7 valid loss: tensor(0.1710, device='cuda:0') f1_micro: 0.6235294117647059 f1_macro: 0.14911218340620344
Epoch 8 train loss: tensor(0.1009, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 8 valid loss: tensor(0.1778, device='cuda:0') f1_micro: 0.6203703703703705 f1_macro: 0.15275382691235798
Epoch 9 train loss: tensor(0.0912, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 9 valid loss: tensor(0.1805, device='cuda:0') f1_micro: 0.6245530393325388 f1_macro: 0.15068991100622314
Epoch 10 train loss: tensor(0.0805, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 10 valid loss: tensor(0.1917, device='cuda:0') f1_micro: 0.6033254156769596 f1_macro: 0.1412759960761027
Epoch 11 train loss: tensor(0.0766, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 11 valid loss: tensor(0.2050, device='cuda:0') f1_micro: 0.5842956120092379 f1_macro: 0.1408346787708838
Epoch 12 train loss: tensor(0.0706, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 12 valid loss: tensor(0.2005, device='cuda:0') f1_micro: 0.5945303210463734 f1_macro: 0.14247011461534637
Epoch 13 train loss: tensor(0.0649, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 13 valid loss: tensor(0.2177, device='cuda:0') f1_micro: 0.6248548199767711 f1_macro: 0.13662293778572848
Epoch 14 train loss: tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 14 valid loss: tensor(0.2182, device='cuda:0') f1_micro: 0.5976470588235293 f1_macro: 0.1496806022034531
Epoch 15 train loss: tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 15 valid loss: tensor(0.2380, device='cuda:0') f1_micro: 0.5944700460829493 f1_macro: 0.1275171649687543
Epoch 16 train loss: tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 16 valid loss: tensor(0.2119, device='cuda:0') f1_micro: 0.6164542294322132 f1_macro: 0.17798872088749812
Saving Best
Epoch 17 train loss: tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 17 valid loss: tensor(0.2462, device='cuda:0') f1_micro: 0.6186726659167604 f1_macro: 0.14938888872480863
Epoch 18 train loss: tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 18 valid loss: tensor(0.2381, device='cuda:0') f1_micro: 0.6073903002309469 f1_macro: 0.16256904473024766
Epoch 19 train loss: tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 19 valid loss: tensor(0.2223, device='cuda:0') f1_micro: 0.5974625144175317 f1_macro: 0.19545879471240607
Saving Best
Epoch 20 train loss: tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 20 valid loss: tensor(0.2597, device='cuda:0') f1_micro: 0.5927654609101516 f1_macro: 0.15810173047415285
Epoch 21 train loss: tensor(0.0366, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 21 valid loss: tensor(0.2561, device='cuda:0') f1_micro: 0.5787709497206703 f1_macro: 0.17629834409547132
Epoch 22 train loss: tensor(0.0377, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 22 valid loss: tensor(0.2677, device='cuda:0') f1_micro: 0.6370699223085461 f1_macro: 0.158771761393227
Epoch 23 train loss: tensor(0.0328, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 23 valid loss: tensor(0.2476, device='cuda:0') f1_micro: 0.6238738738738739 f1_macro: 0.19338419033540985
Epoch 24 train loss: tensor(0.0338, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 24 valid loss: tensor(0.2775, device='cuda:0') f1_micro: 0.6220204313280362 f1_macro: 0.16985965852049803
Epoch 25 train loss: tensor(0.0299, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 25 valid loss: tensor(0.2439, device='cuda:0') f1_micro: 0.6087990487514863 f1_macro: 0.17535725963180276
Epoch 26 train loss: tensor(0.0316, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 26 valid loss: tensor(0.2544, device='cuda:0') f1_micro: 0.6141552511415526 f1_macro: 0.1611347936742846
Epoch 27 train loss: tensor(0.0246, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 27 valid loss: tensor(0.2933, device='cuda:0') f1_micro: 0.6380090497737557 f1_macro: 0.1501910578816442
Epoch 28 train loss: tensor(0.0251, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 28 valid loss: tensor(0.2698, device='cuda:0') f1_micro: 0.5843230403800476 f1_macro: 0.15472867324240863
Epoch 29 train loss: tensor(0.0228, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 29 valid loss: tensor(0.2940, device='cuda:0') f1_micro: 0.5923344947735192 f1_macro: 0.1497588543287722
Epoch 30 train loss: tensor(0.0234, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 30 valid loss: tensor(0.2896, device='cuda:0') f1_micro: 0.6300114547537228 f1_macro: 0.16571621959820715
Epoch 31 train loss: tensor(0.0226, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 31 valid loss: tensor(0.2904, device='cuda:0') f1_micro: 0.5861244019138755 f1_macro: 0.15282412518244087
Epoch 32 train loss: tensor(0.0237, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 32 valid loss: tensor(0.3056, device='cuda:0') f1_micro: 0.6033898305084746 f1_macro: 0.15634521921325312
Epoch 33 train loss: tensor(0.0229, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 33 valid loss: tensor(0.3042, device='cuda:0') f1_micro: 0.6232876712328768 f1_macro: 0.14949707931429143
Epoch 34 train loss: tensor(0.0204, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 34 valid loss: tensor(0.2796, device='cuda:0') f1_micro: 0.6274509803921569 f1_macro: 0.1506079783329528
Epoch 35 train loss: tensor(0.0186, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 35 valid loss: tensor(0.3056, device='cuda:0') f1_micro: 0.5936794582392777 f1_macro: 0.1501209544524472
Epoch 36 train loss: tensor(0.0196, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 36 valid loss: tensor(0.2745, device='cuda:0') f1_micro: 0.5951557093425606 f1_macro: 0.15887417546844765
Epoch 37 train loss: tensor(0.0198, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 37 valid loss: tensor(0.2928, device='cuda:0') f1_micro: 0.5990453460620525 f1_macro: 0.15342306857952123
Epoch 38 train loss: tensor(0.0172, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 38 valid loss: tensor(0.3010, device='cuda:0') f1_micro: 0.6309794988610478 f1_macro: 0.17004199659786576
Epoch 39 train loss: tensor(0.0188, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 39 valid loss: tensor(0.2956, device='cuda:0') f1_micro: 0.6170921198668146 f1_macro: 0.18700797789780838
Epoch 40 train loss: tensor(0.0160, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 40 valid loss: tensor(0.3427, device='cuda:0') f1_micro: 0.6094117647058823 f1_macro: 0.15301387919118234
Epoch 41 train loss: tensor(0.0156, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 41 valid loss: tensor(0.3112, device='cuda:0') f1_micro: 0.6181818181818182 f1_macro: 0.16984390763406645
Epoch 42 train loss: tensor(0.0159, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 42 valid loss: tensor(0.3367, device='cuda:0') f1_micro: 0.605080831408776 f1_macro: 0.12758818878370146
Epoch 43 train loss: tensor(0.0142, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 43 valid loss: tensor(0.3264, device='cuda:0') f1_micro: 0.6419753086419753 f1_macro: 0.1667053131538166
Epoch 44 train loss: tensor(0.0167, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 44 valid loss: tensor(0.3116, device='cuda:0') f1_micro: 0.5810055865921788 f1_macro: 0.1534061254771613
Epoch 45 train loss: tensor(0.0159, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 45 valid loss: tensor(0.3361, device='cuda:0') f1_micro: 0.6256983240223465 f1_macro: 0.17880590534085974
Epoch 46 train loss: tensor(0.0141, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 46 valid loss: tensor(0.3539, device='cuda:0') f1_micro: 0.6073903002309469 f1_macro: 0.1200203432131491
Epoch 47 train loss: tensor(0.0160, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 47 valid loss: tensor(0.3402, device='cuda:0') f1_micro: 0.630359212050985 f1_macro: 0.14866419149678842
Epoch 48 train loss: tensor(0.0138, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 48 valid loss: tensor(0.3291, device='cuda:0') f1_micro: 0.6329113924050633 f1_macro: 0.16803492074346701
Epoch 49 train loss: tensor(0.0136, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 49 valid loss: tensor(0.3349, device='cuda:0') f1_micro: 0.624263839811543 f1_macro: 0.14146632429195455
Epoch 50 train loss: tensor(0.0113, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 50 valid loss: tensor(0.3432, device='cuda:0') f1_micro: 0.6020761245674742 f1_macro: 0.13894506380894764
Epoch 51 train loss: tensor(0.0139, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 51 valid loss: tensor(0.3368, device='cuda:0') f1_micro: 0.6144859813084113 f1_macro: 0.15976425514853637
Epoch 52 train loss: tensor(0.0121, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 52 valid loss: tensor(0.3630, device='cuda:0') f1_micro: 0.5943502824858757 f1_macro: 0.13025532517141833
Epoch 53 train loss: tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 53 valid loss: tensor(0.3743, device='cuda:0') f1_micro: 0.6311010215664018 f1_macro: 0.1448785286050971
Epoch 54 train loss: tensor(0.0119, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 54 valid loss: tensor(0.3709, device='cuda:0') f1_micro: 0.6050228310502282 f1_macro: 0.1247269732804065
Epoch 55 train loss: tensor(0.0130, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 55 valid loss: tensor(0.3856, device='cuda:0') f1_micro: 0.5936794582392777 f1_macro: 0.13751339431539084
Epoch 56 train loss: tensor(0.0127, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 56 valid loss: tensor(0.3804, device='cuda:0') f1_micro: 0.6223776223776224 f1_macro: 0.1460751903848108
Epoch 57 train loss: tensor(0.0109, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 57 valid loss: tensor(0.3583, device='cuda:0') f1_micro: 0.6197802197802198 f1_macro: 0.15737672905075367
Epoch 58 train loss: tensor(0.0112, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 58 valid loss: tensor(0.3389, device='cuda:0') f1_micro: 0.6199095022624433 f1_macro: 0.15863018038951524
Epoch 59 train loss: tensor(0.0119, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 59 valid loss: tensor(0.3624, device='cuda:0') f1_micro: 0.587709497206704 f1_macro: 0.12867852268294516
Epoch 60 train loss: tensor(0.0111, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 60 valid loss: tensor(0.3601, device='cuda:0') f1_micro: 0.5970149253731344 f1_macro: 0.13962771169426424
Epoch 61 train loss: tensor(0.0120, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 61 valid loss: tensor(0.3405, device='cuda:0') f1_micro: 0.6192090395480226 f1_macro: 0.15718536614721176
Epoch 62 train loss: tensor(0.0109, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 62 valid loss: tensor(0.3675, device='cuda:0') f1_micro: 0.623728813559322 f1_macro: 0.16084047872636037
Epoch 63 train loss: tensor(0.0099, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 63 valid loss: tensor(0.3474, device='cuda:0') f1_micro: 0.6425339366515836 f1_macro: 0.1522610004324049
Epoch 64 train loss: tensor(0.0122, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 64 valid loss: tensor(0.3863, device='cuda:0') f1_micro: 0.6042841037204059 f1_macro: 0.1405446359956615
Epoch 65 train loss: tensor(0.0106, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 65 valid loss: tensor(0.3741, device='cuda:0') f1_micro: 0.6356413166855847 f1_macro: 0.1526082417283019
Epoch 66 train loss: tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 66 valid loss: tensor(0.3342, device='cuda:0') f1_micro: 0.6176808266360505 f1_macro: 0.15871641585927296
Epoch 67 train loss: tensor(0.0117, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 67 valid loss: tensor(0.3897, device='cuda:0') f1_micro: 0.6064073226544622 f1_macro: 0.14299574215893732
Epoch 68 train loss: tensor(0.0099, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 68 valid loss: tensor(0.3810, device='cuda:0') f1_micro: 0.6162657502863688 f1_macro: 0.1299794299468973
Epoch 69 train loss: tensor(0.0103, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 69 valid loss: tensor(0.3476, device='cuda:0') f1_micro: 0.630359212050985 f1_macro: 0.15017383344967214
Epoch 70 train loss: tensor(0.0113, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 70 valid loss: tensor(0.3689, device='cuda:0') f1_micro: 0.5979843225083986 f1_macro: 0.15441389126271976
Epoch 71 train loss: tensor(0.0089, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 71 valid loss: tensor(0.3854, device='cuda:0') f1_micro: 0.5852585258525853 f1_macro: 0.14759215958182273
Epoch 72 train loss: tensor(0.0103, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 72 valid loss: tensor(0.3614, device='cuda:0') f1_micro: 0.5797101449275361 f1_macro: 0.15158497405503343
Epoch 73 train loss: tensor(0.0075, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 73 valid loss: tensor(0.3620, device='cuda:0') f1_micro: 0.6057142857142858 f1_macro: 0.15598065022339833
Epoch 74 train loss: tensor(0.0095, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 74 valid loss: tensor(0.3574, device='cuda:0') f1_micro: 0.64472190692395 f1_macro: 0.17536879540285108
Epoch 75 train loss: tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 75 valid loss: tensor(0.3743, device='cuda:0') f1_micro: 0.6325369738339021 f1_macro: 0.15769355414133865
Epoch 76 train loss: tensor(0.0090, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 76 valid loss: tensor(0.3344, device='cuda:0') f1_micro: 0.647191011235955 f1_macro: 0.16242126480221716
Epoch 77 train loss: tensor(0.0074, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 77 valid loss: tensor(0.3913, device='cuda:0') f1_micro: 0.6417233560090703 f1_macro: 0.1622870691994103
Epoch 78 train loss: tensor(0.0090, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 78 valid loss: tensor(0.4382, device='cuda:0') f1_micro: 0.6206088992974239 f1_macro: 0.11441042820514712
Epoch 79 train loss: tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 79 valid loss: tensor(0.3791, device='cuda:0') f1_micro: 0.6188850967007963 f1_macro: 0.17280441711314615
Epoch 80 train loss: tensor(0.0080, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 80 valid loss: tensor(0.3748, device='cuda:0') f1_micro: 0.6263982102908278 f1_macro: 0.14699664813495633
Epoch 81 train loss: tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 81 valid loss: tensor(0.4056, device='cuda:0') f1_micro: 0.6321709786276716 f1_macro: 0.1535444794851984
Epoch 82 train loss: tensor(0.0073, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 82 valid loss: tensor(0.3817, device='cuda:0') f1_micro: 0.6425339366515836 f1_macro: 0.1511538335194355
Epoch 83 train loss: tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 83 valid loss: tensor(0.4335, device='cuda:0') f1_micro: 0.6252821670428895 f1_macro: 0.13386358193756603
Epoch 84 train loss: tensor(0.0109, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 84 valid loss: tensor(0.3775, device='cuda:0') f1_micro: 0.5864978902953587 f1_macro: 0.19295681795699465
Epoch 85 train loss: tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 85 valid loss: tensor(0.3802, device='cuda:0') f1_micro: 0.6221719457013575 f1_macro: 0.15221629521124785
Epoch 86 train loss: tensor(0.0071, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 86 valid loss: tensor(0.3589, device='cuda:0') f1_micro: 0.5913621262458473 f1_macro: 0.1610873701333763
Epoch 87 train loss: tensor(0.0078, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 87 valid loss: tensor(0.4142, device='cuda:0') f1_micro: 0.6025492468134415 f1_macro: 0.11487466811511708
Epoch 88 train loss: tensor(0.0079, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 88 valid loss: tensor(0.3665, device='cuda:0') f1_micro: 0.6503496503496504 f1_macro: 0.1390841284950239
Epoch 89 train loss: tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 89 valid loss: tensor(0.4165, device='cuda:0') f1_micro: 0.5794392523364486 f1_macro: 0.13811712466197518
Epoch 90 train loss: tensor(0.0057, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 90 valid loss: tensor(0.3588, device='cuda:0') f1_micro: 0.6209223847019122 f1_macro: 0.1532773563921776
Epoch 91 train loss: tensor(0.0067, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 91 valid loss: tensor(0.3748, device='cuda:0') f1_micro: 0.5926800472255017 f1_macro: 0.15157097926890056
Epoch 92 train loss: tensor(0.0080, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 92 valid loss: tensor(0.4175, device='cuda:0') f1_micro: 0.619158878504673 f1_macro: 0.12025522574621246
Epoch 93 train loss: tensor(0.0078, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 93 valid loss: tensor(0.4068, device='cuda:0') f1_micro: 0.6112956810631229 f1_macro: 0.16002110134763198
Epoch 94 train loss: tensor(0.0058, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 94 valid loss: tensor(0.3765, device='cuda:0') f1_micro: 0.6166281755196306 f1_macro: 0.1656593574398956
Epoch 95 train loss: tensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 95 valid loss: tensor(0.4019, device='cuda:0') f1_micro: 0.6232876712328768 f1_macro: 0.15676441379464293
Epoch 96 train loss: tensor(0.0060, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 96 valid loss: tensor(0.4313, device='cuda:0') f1_micro: 0.6064814814814814 f1_macro: 0.11895399615797124
Epoch 97 train loss: tensor(0.0073, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 97 valid loss: tensor(0.3898, device='cuda:0') f1_micro: 0.6022988505747127 f1_macro: 0.1500816133428229
Epoch 98 train loss: tensor(0.0060, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 98 valid loss: tensor(0.4381, device='cuda:0') f1_micro: 0.5886363636363636 f1_macro: 0.12274637760871136
Epoch 99 train loss: tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 99 valid loss: tensor(0.4588, device='cuda:0') f1_micro: 0.6194285714285714 f1_macro: 0.13944608740897968
Training Time: 714.0560116767883
Training Peak Mem: 2885.859375
Training Params: 15718158
Testing model ./log/test_chestx/model_image.pt and ./log/test_chestx/head_image.pt:
 f1_micro: 0.5569060773480663 f1_macro: 0.1731634011401613
Inference Time: 1.061922311782837
Inference Params: 15718158
