Loaded split indices from split_indices.pth
Creating noisy data loaders with consistent indices
Loaded existing noise indices for train split from /home/bianca/Code/MultiBench/noise_indices/train_35444f0a84540ad129b5b7ac130d66c9_2941_64.json
Loaded existing noise indices for val split from /home/bianca/Code/MultiBench/noise_indices/val_35444f0a84540ad129b5b7ac130d66c9_367_64.json
Loaded existing noise indices for test split from /home/bianca/Code/MultiBench/noise_indices/test_35444f0a84540ad129b5b7ac130d66c9_369_64.json
mean branch weight 0.4189, 0.5811
--------------------------------------------------
Epoch 0 | Train loss 0.1148 | Train CE loss 0.0860 | Val loss 0.2075 | patience 0
f1 micro: 0.882 | f1 macro: 0.668 
Saving Best
mean branch weight 0.4757, 0.5243
--------------------------------------------------
Epoch 1 | Train loss 0.1094 | Train CE loss 0.0853 | Val loss 0.2005 | patience 0
f1 micro: 0.883 | f1 macro: 0.664 
mean branch weight 0.5172, 0.4828
--------------------------------------------------
Epoch 2 | Train loss 0.1051 | Train CE loss 0.0790 | Val loss 0.1982 | patience 1
f1 micro: 0.881 | f1 macro: 0.670 
Saving Best
mean branch weight 0.5117, 0.4883
--------------------------------------------------
Epoch 3 | Train loss 0.1086 | Train CE loss 0.0844 | Val loss 0.1949 | patience 0
f1 micro: 0.887 | f1 macro: 0.679 
Saving Best
mean branch weight 0.5565, 0.4435
--------------------------------------------------
Epoch 4 | Train loss 0.1039 | Train CE loss 0.0806 | Val loss 0.1906 | patience 0
f1 micro: 0.884 | f1 macro: 0.658 
mean branch weight 0.5538, 0.4462
--------------------------------------------------
Epoch 5 | Train loss 0.1021 | Train CE loss 0.0795 | Val loss 0.1956 | patience 1
f1 micro: 0.884 | f1 macro: 0.681 
Saving Best
mean branch weight 0.4981, 0.5019
--------------------------------------------------
Epoch 6 | Train loss 0.1065 | Train CE loss 0.0848 | Val loss 0.2042 | patience 0
f1 micro: 0.884 | f1 macro: 0.685 
Saving Best
mean branch weight 0.5210, 0.4790
--------------------------------------------------
Epoch 7 | Train loss 0.1001 | Train CE loss 0.0780 | Val loss 0.2006 | patience 0
f1 micro: 0.883 | f1 macro: 0.666 
mean branch weight 0.5211, 0.4789
--------------------------------------------------
Epoch 8 | Train loss 0.0981 | Train CE loss 0.0760 | Val loss 0.2004 | patience 1
f1 micro: 0.882 | f1 macro: 0.665 
mean branch weight 0.5328, 0.4672
--------------------------------------------------
Epoch 9 | Train loss 0.0924 | Train CE loss 0.0690 | Val loss 0.2100 | patience 2
f1 micro: 0.885 | f1 macro: 0.681 
mean branch weight 0.5320, 0.4680
--------------------------------------------------
Epoch 10 | Train loss 0.1019 | Train CE loss 0.0807 | Val loss 0.2047 | patience 3
f1 micro: 0.886 | f1 macro: 0.679 
mean branch weight 0.5592, 0.4408
--------------------------------------------------
Epoch 11 | Train loss 0.0967 | Train CE loss 0.0748 | Val loss 0.2005 | patience 4
f1 micro: 0.886 | f1 macro: 0.683 
mean branch weight 0.5986, 0.4014
--------------------------------------------------
Epoch 12 | Train loss 0.0928 | Train CE loss 0.0717 | Val loss 0.1971 | patience 5
f1 micro: 0.883 | f1 macro: 0.662 
mean branch weight 0.5320, 0.4680
--------------------------------------------------
Epoch 13 | Train loss 0.0943 | Train CE loss 0.0734 | Val loss 0.2073 | patience 6
f1 micro: 0.888 | f1 macro: 0.682 
mean branch weight 0.5228, 0.4772
--------------------------------------------------
Epoch 14 | Train loss 0.0923 | Train CE loss 0.0707 | Val loss 0.2134 | patience 7
f1 micro: 0.883 | f1 macro: 0.679 
Training Time: 274.1256191730499
Training Peak Mem: 2890.8828125
Training Params: 57920814
Testing model ./log/test_chestx/DynMMNet_freeze_qTrue_reg_0.05_noise_gaussian_50.pt:
------------------------------Test data------------------------------
f1_micro: 86.03 | f1_macro: 68.30
Branch selection statistics:
Branch 1: selected 177.0 times (47.97% of samples)
Branch 2: selected 192.0 times (52.03% of samples)

mean branch weight 0.4797, 0.5203
0.5203251838684082
Total Flops 12.41M
0.8602540834845734 0.6829720475123867 12.406782150268555
