Loaded split indices from split_indices.pth
Creating noisy data loaders with consistent indices
Loaded existing noise indices for train split from /home/bianca/Code/MultiBench/noise_indices/train_afd7ae2effcdcbfc4bca17a9eb2ecd09_2941_64.json
Loaded existing noise indices for val split from /home/bianca/Code/MultiBench/noise_indices/val_afd7ae2effcdcbfc4bca17a9eb2ecd09_367_64.json
Loaded existing noise indices for test split from /home/bianca/Code/MultiBench/noise_indices/test_afd7ae2effcdcbfc4bca17a9eb2ecd09_369_64.json
mean branch weight 0.5038, 0.4962
--------------------------------------------------
Epoch 0 | Train loss 0.0488 | Train CE loss 0.0193 | Val loss 0.2161 | patience 0
f1 micro: 0.895 | f1 macro: 0.697 
Saving Best
mean branch weight 0.4347, 0.5653
--------------------------------------------------
Epoch 1 | Train loss 0.0437 | Train CE loss 0.0193 | Val loss 0.2297 | patience 0
f1 micro: 0.893 | f1 macro: 0.698 
Saving Best
mean branch weight 0.4860, 0.5140
--------------------------------------------------
Epoch 2 | Train loss 0.0425 | Train CE loss 0.0179 | Val loss 0.2226 | patience 0
f1 micro: 0.892 | f1 macro: 0.682 
mean branch weight 0.5090, 0.4910
--------------------------------------------------
Epoch 3 | Train loss 0.0425 | Train CE loss 0.0189 | Val loss 0.2246 | patience 1
f1 micro: 0.888 | f1 macro: 0.674 
mean branch weight 0.5405, 0.4595
--------------------------------------------------
Epoch 4 | Train loss 0.0409 | Train CE loss 0.0180 | Val loss 0.2192 | patience 2
f1 micro: 0.888 | f1 macro: 0.688 
mean branch weight 0.4735, 0.5265
--------------------------------------------------
Epoch 5 | Train loss 0.0411 | Train CE loss 0.0183 | Val loss 0.2269 | patience 3
f1 micro: 0.894 | f1 macro: 0.700 
Saving Best
mean branch weight 0.4967, 0.5033
--------------------------------------------------
Epoch 6 | Train loss 0.0405 | Train CE loss 0.0182 | Val loss 0.2236 | patience 0
f1 micro: 0.885 | f1 macro: 0.664 
mean branch weight 0.5317, 0.4683
--------------------------------------------------
Epoch 7 | Train loss 0.0404 | Train CE loss 0.0178 | Val loss 0.2205 | patience 1
f1 micro: 0.888 | f1 macro: 0.680 
mean branch weight 0.5489, 0.4511
--------------------------------------------------
Epoch 8 | Train loss 0.0410 | Train CE loss 0.0189 | Val loss 0.2175 | patience 2
f1 micro: 0.891 | f1 macro: 0.688 
mean branch weight 0.5395, 0.4605
--------------------------------------------------
Epoch 9 | Train loss 0.0397 | Train CE loss 0.0178 | Val loss 0.2226 | patience 3
f1 micro: 0.891 | f1 macro: 0.697 
mean branch weight 0.5473, 0.4527
--------------------------------------------------
Epoch 10 | Train loss 0.0405 | Train CE loss 0.0185 | Val loss 0.2173 | patience 4
f1 micro: 0.891 | f1 macro: 0.675 
mean branch weight 0.5385, 0.4615
--------------------------------------------------
Epoch 11 | Train loss 0.0394 | Train CE loss 0.0177 | Val loss 0.2237 | patience 5
f1 micro: 0.892 | f1 macro: 0.688 
mean branch weight 0.5418, 0.4582
--------------------------------------------------
Epoch 12 | Train loss 0.0401 | Train CE loss 0.0184 | Val loss 0.2172 | patience 6
f1 micro: 0.889 | f1 macro: 0.672 
mean branch weight 0.5445, 0.4555
--------------------------------------------------
Epoch 13 | Train loss 0.0397 | Train CE loss 0.0178 | Val loss 0.2168 | patience 7
f1 micro: 0.895 | f1 macro: 0.681 
Training Time: 254.4288649559021
Training Peak Mem: 2875.21484375
Training Params: 57920814
Testing model ./log/test_chestx/DynMMNet_freeze_qTrue_reg_0.05_noise_mask_01_2.pt:
------------------------------Test data------------------------------
f1_micro: 86.64 | f1_macro: 66.74
Branch selection statistics:
Branch 1: selected 179.0 times (48.51% of samples)
Branch 2: selected 190.0 times (51.49% of samples)

mean branch weight 0.4851, 0.5149
0.5149051547050476
Total Flops 12.30M
0.8663594470046083 0.6673692401508703 12.300311088562012
